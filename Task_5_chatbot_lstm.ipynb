{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "id": "Dxa8__xH10Hw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lha kaggle.json\n",
        "\n",
        "!pip install -q Kaggle\n",
        "\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "9bkxmRdeTSi-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "855d45ee-eb75-48c0-ab8c-7ab203d674b4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw-r--r-- 1 root root 66 May 29 00:54 kaggle.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d kausr25/chatterbotenglish\n",
        "\n",
        "!unzip chatterbotenglish.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGx4BGiR1-20",
        "outputId": "f3bfe498-0010-401b-805e-74e9870a9d71"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/kausr25/chatterbotenglish\n",
            "License(s): unknown\n",
            "Downloading chatterbotenglish.zip to /content\n",
            "  0% 0.00/23.2k [00:00<?, ?B/s]\n",
            "100% 23.2k/23.2k [00:00<00:00, 43.0MB/s]\n",
            "Archive:  chatterbotenglish.zip\n",
            "  inflating: ai.yml                  \n",
            "  inflating: botprofile.yml          \n",
            "  inflating: computers.yml           \n",
            "  inflating: emotion.yml             \n",
            "  inflating: food.yml                \n",
            "  inflating: gossip.yml              \n",
            "  inflating: greetings.yml           \n",
            "  inflating: health.yml              \n",
            "  inflating: history.yml             \n",
            "  inflating: humor.yml               \n",
            "  inflating: literature.yml          \n",
            "  inflating: money.yml               \n",
            "  inflating: movies.yml              \n",
            "  inflating: politics.yml            \n",
            "  inflating: psychology.yml          \n",
            "  inflating: science.yml             \n",
            "  inflating: sports.yml              \n",
            "  inflating: trivia.yml              \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "import os\n",
        "\n",
        "dir_path = '/content'\n",
        "data = os.listdir(dir_path + os.sep)"
      ],
      "metadata": {
        "id": "myQM7oF1S_r1"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z3kZC8BE2OZu",
        "outputId": "78768cfd-a120-43b1-c2be-e8832b1c9b79"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.config',\n",
              " 'botprofile.yml',\n",
              " 'gossip.yml',\n",
              " 'humor.yml',\n",
              " 'movies.yml',\n",
              " 'ai.yml',\n",
              " 'literature.yml',\n",
              " 'psychology.yml',\n",
              " 'sports.yml',\n",
              " 'greetings.yml',\n",
              " 'computers.yml',\n",
              " 'food.yml',\n",
              " 'kaggle.json',\n",
              " 'trivia.yml',\n",
              " 'emotion.yml',\n",
              " 'money.yml',\n",
              " 'politics.yml',\n",
              " 'science.yml',\n",
              " 'health.yml',\n",
              " 'chatterbotenglish.zip',\n",
              " 'history.yml',\n",
              " 'sample_data']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "files_list = []\n",
        "for i in data:\n",
        "  if i.endswith(('.yml', '.yaml')):\n",
        "    files_list.append(i)"
      ],
      "metadata": {
        "id": "slG-4l_i2VQk"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kP7jj1wk2hAn",
        "outputId": "f7776f8e-3778-4648-b06c-cc5568168f0a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['botprofile.yml',\n",
              " 'gossip.yml',\n",
              " 'humor.yml',\n",
              " 'movies.yml',\n",
              " 'ai.yml',\n",
              " 'literature.yml',\n",
              " 'psychology.yml',\n",
              " 'sports.yml',\n",
              " 'greetings.yml',\n",
              " 'computers.yml',\n",
              " 'food.yml',\n",
              " 'trivia.yml',\n",
              " 'emotion.yml',\n",
              " 'money.yml',\n",
              " 'politics.yml',\n",
              " 'science.yml',\n",
              " 'health.yml',\n",
              " 'history.yml']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "hAukahnlS-ZP"
      },
      "outputs": [],
      "source": [
        "questions, answers = [], []\n",
        "\n",
        "for filepath in files_list:\n",
        "    file_ = open(dir_path + os.sep + filepath , 'rb')\n",
        "    docs = yaml.safe_load(file_)\n",
        "    conversations = docs['conversations']\n",
        "    for con in conversations:\n",
        "        if len(con) > 2 :\n",
        "            questions.append(con[0])\n",
        "            replies = con[1 :]\n",
        "            ans = ''\n",
        "            for rep in replies:\n",
        "                ans += ' ' + rep\n",
        "            answers.append(ans)\n",
        "        elif len(con)> 1:\n",
        "            questions.append(con[0])\n",
        "            answers.append(con[1])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "questions"
      ],
      "metadata": {
        "id": "l-_CImQS2oOG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "G239vRms3THG"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers , activations , models , preprocessing, utils"
      ],
      "metadata": {
        "id": "aPHkJf652zT-"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answers_with_tags = []\n",
        "for i in range(len(answers)):\n",
        "    if type(answers[i]) == str:\n",
        "        answers_with_tags.append(answers[i])\n",
        "    else:\n",
        "        questions.pop(i)\n",
        "\n",
        "answers = []\n",
        "for i in range(len(answers_with_tags)) :\n",
        "    answers.append('<START> ' + answers_with_tags[i] + ' <END>')\n",
        "\n",
        "tokenizer = preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts(questions + answers)\n",
        "VOCAB_SIZE = len(tokenizer.word_index)+1"
      ],
      "metadata": {
        "id": "jAzobEPP2phK"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "import re\n",
        "\n",
        "vocab = []\n",
        "for word in tokenizer.word_index:\n",
        "    vocab.append(word)\n",
        "\n",
        "def tokenize(sentences):\n",
        "    tokens_list = []\n",
        "    vocabulary = []\n",
        "    for sentence in sentences:\n",
        "        sentence = sentence.lower()\n",
        "        sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
        "        tokens = sentence.split()\n",
        "        vocabulary += tokens\n",
        "        tokens_list.append(tokens)\n",
        "    return tokens_list , vocabulary"
      ],
      "metadata": {
        "id": "3t-JIqco245H"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_questions = tokenizer.texts_to_sequences(questions)\n",
        "maxlen_questions = max([len(x) for x in tokenized_questions])\n",
        "padded_questions = preprocessing.sequence.pad_sequences(tokenized_questions , maxlen=maxlen_questions , padding='post')\n",
        "encoder_input_data = np.array(padded_questions)"
      ],
      "metadata": {
        "id": "btgZ8uXv3RNg"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_answers = tokenizer.texts_to_sequences(answers)\n",
        "maxlen_answers = max([len(x) for x in tokenized_answers])\n",
        "padded_answers = preprocessing.sequence.pad_sequences(tokenized_answers , maxlen=maxlen_answers , padding='post')\n",
        "decoder_input_data = np.array(padded_answers)"
      ],
      "metadata": {
        "id": "4U5UygtT3X1j"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_answers = tokenizer.texts_to_sequences(answers)\n",
        "for i in range(len(tokenized_answers)) :\n",
        "    tokenized_answers[i] = tokenized_answers[i][1:]\n",
        "padded_answers = preprocessing.sequence.pad_sequences(tokenized_answers , maxlen=maxlen_answers , padding='post')\n",
        "onehot_answers = utils.to_categorical(padded_answers , VOCAB_SIZE)\n",
        "decoder_output_data = np.array(onehot_answers)"
      ],
      "metadata": {
        "id": "svdW49uw3esQ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "jI-qvVco3juH"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_inputs = tf.keras.layers.Input(shape=(maxlen_questions ,))\n",
        "encoder_embedding = tf.keras.layers.Embedding(VOCAB_SIZE, 200 , mask_zero=True) (encoder_inputs)\n",
        "encoder_outputs , state_h , state_c = tf.keras.layers.LSTM(200 , return_state=True)(encoder_embedding)\n",
        "encoder_states = [ state_h , state_c ]\n",
        "\n",
        "decoder_inputs = tf.keras.layers.Input(shape=(maxlen_answers , ))\n",
        "decoder_embedding = tf.keras.layers.Embedding(VOCAB_SIZE, 200 , mask_zero=True) (decoder_inputs)\n",
        "decoder_lstm = tf.keras.layers.LSTM(200 , return_state=True , return_sequences=True)\n",
        "decoder_outputs , _ , _ = decoder_lstm (decoder_embedding , initial_state=encoder_states)\n",
        "decoder_dense = tf.keras.layers.Dense(VOCAB_SIZE , activation=tf.keras.activations.softmax)\n",
        "output = decoder_dense (decoder_outputs)\n",
        "\n",
        "model = tf.keras.models.Model([encoder_inputs, decoder_inputs], output)"
      ],
      "metadata": {
        "id": "DNGfs27Y3id7"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.RMSprop(), loss='categorical_crossentropy')"
      ],
      "metadata": {
        "id": "dl6wGV5FNyGh"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3upYFhuNzWN",
        "outputId": "3fbea5b4-4adc-4a66-f595-e3b84b01fb78"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)        [(None, 22)]                 0         []                            \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)        [(None, 74)]                 0         []                            \n",
            "                                                                                                  \n",
            " embedding (Embedding)       (None, 22, 200)              378800    ['input_1[0][0]']             \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)     (None, 74, 200)              378800    ['input_2[0][0]']             \n",
            "                                                                                                  \n",
            " lstm (LSTM)                 [(None, 200),                320800    ['embedding[0][0]']           \n",
            "                              (None, 200),                                                        \n",
            "                              (None, 200)]                                                        \n",
            "                                                                                                  \n",
            " lstm_1 (LSTM)               [(None, 74, 200),            320800    ['embedding_1[0][0]',         \n",
            "                              (None, 200),                           'lstm[0][1]',                \n",
            "                              (None, 200)]                           'lstm[0][2]']                \n",
            "                                                                                                  \n",
            " dense (Dense)               (None, 74, 1894)             380694    ['lstm_1[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1779894 (6.79 MB)\n",
            "Trainable params: 1779894 (6.79 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit([encoder_input_data , decoder_input_data], decoder_output_data, batch_size=32, epochs=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ueqz-bNoN9gb",
        "outputId": "ca3cdffb-6d27-4cea-ddc6-4effccf4e0d7"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "18/18 [==============================] - 8s 464ms/step - loss: 4.2243\n",
            "Epoch 2/100\n",
            "18/18 [==============================] - 7s 390ms/step - loss: 4.1906\n",
            "Epoch 3/100\n",
            "18/18 [==============================] - 8s 465ms/step - loss: 4.1603\n",
            "Epoch 4/100\n",
            "18/18 [==============================] - 7s 367ms/step - loss: 4.1328\n",
            "Epoch 5/100\n",
            "18/18 [==============================] - 8s 462ms/step - loss: 4.1088\n",
            "Epoch 6/100\n",
            "18/18 [==============================] - 7s 367ms/step - loss: 4.0626\n",
            "Epoch 7/100\n",
            "18/18 [==============================] - 8s 464ms/step - loss: 4.0389\n",
            "Epoch 8/100\n",
            "18/18 [==============================] - 7s 375ms/step - loss: 4.0188\n",
            "Epoch 9/100\n",
            "18/18 [==============================] - 8s 440ms/step - loss: 3.9917\n",
            "Epoch 10/100\n",
            "18/18 [==============================] - 8s 429ms/step - loss: 3.9757\n",
            "Epoch 11/100\n",
            "18/18 [==============================] - 7s 386ms/step - loss: 3.9490\n",
            "Epoch 12/100\n",
            "18/18 [==============================] - 8s 462ms/step - loss: 3.9360\n",
            "Epoch 13/100\n",
            "18/18 [==============================] - 7s 379ms/step - loss: 3.9097\n",
            "Epoch 14/100\n",
            "18/18 [==============================] - 8s 454ms/step - loss: 3.8797\n",
            "Epoch 15/100\n",
            "18/18 [==============================] - 7s 376ms/step - loss: 3.8593\n",
            "Epoch 16/100\n",
            "18/18 [==============================] - 8s 464ms/step - loss: 3.8235\n",
            "Epoch 17/100\n",
            "18/18 [==============================] - 7s 402ms/step - loss: 3.8060\n",
            "Epoch 18/100\n",
            "18/18 [==============================] - 8s 428ms/step - loss: 3.7946\n",
            "Epoch 19/100\n",
            "18/18 [==============================] - 8s 450ms/step - loss: 3.7611\n",
            "Epoch 20/100\n",
            "18/18 [==============================] - 7s 372ms/step - loss: 3.7390\n",
            "Epoch 21/100\n",
            "18/18 [==============================] - 8s 465ms/step - loss: 3.7120\n",
            "Epoch 22/100\n",
            "18/18 [==============================] - 7s 367ms/step - loss: 3.6861\n",
            "Epoch 23/100\n",
            "18/18 [==============================] - 10s 559ms/step - loss: 3.6702\n",
            "Epoch 24/100\n",
            "18/18 [==============================] - 7s 377ms/step - loss: 3.6416\n",
            "Epoch 25/100\n",
            "18/18 [==============================] - 8s 463ms/step - loss: 3.5953\n",
            "Epoch 26/100\n",
            "18/18 [==============================] - 7s 368ms/step - loss: 3.5974\n",
            "Epoch 27/100\n",
            "18/18 [==============================] - 8s 444ms/step - loss: 3.5733\n",
            "Epoch 28/100\n",
            "18/18 [==============================] - 8s 439ms/step - loss: 3.5532\n",
            "Epoch 29/100\n",
            "18/18 [==============================] - 7s 368ms/step - loss: 3.5217\n",
            "Epoch 30/100\n",
            "18/18 [==============================] - 8s 459ms/step - loss: 3.5141\n",
            "Epoch 31/100\n",
            "18/18 [==============================] - 7s 372ms/step - loss: 3.4902\n",
            "Epoch 32/100\n",
            "18/18 [==============================] - 8s 461ms/step - loss: 3.4438\n",
            "Epoch 33/100\n",
            "18/18 [==============================] - 7s 373ms/step - loss: 3.4333\n",
            "Epoch 34/100\n",
            "18/18 [==============================] - 8s 456ms/step - loss: 3.4193\n",
            "Epoch 35/100\n",
            "18/18 [==============================] - 7s 368ms/step - loss: 3.3931\n",
            "Epoch 36/100\n",
            "18/18 [==============================] - 8s 449ms/step - loss: 3.3731\n",
            "Epoch 37/100\n",
            "18/18 [==============================] - 8s 424ms/step - loss: 3.3573\n",
            "Epoch 38/100\n",
            "18/18 [==============================] - 8s 406ms/step - loss: 3.3408\n",
            "Epoch 39/100\n",
            "18/18 [==============================] - 8s 457ms/step - loss: 3.3050\n",
            "Epoch 40/100\n",
            "18/18 [==============================] - 7s 377ms/step - loss: 3.2842\n",
            "Epoch 41/100\n",
            "18/18 [==============================] - 8s 467ms/step - loss: 3.2658\n",
            "Epoch 42/100\n",
            "18/18 [==============================] - 7s 373ms/step - loss: 3.2360\n",
            "Epoch 43/100\n",
            "18/18 [==============================] - 8s 466ms/step - loss: 3.2262\n",
            "Epoch 44/100\n",
            "18/18 [==============================] - 7s 367ms/step - loss: 3.1837\n",
            "Epoch 45/100\n",
            "18/18 [==============================] - 8s 448ms/step - loss: 3.1849\n",
            "Epoch 46/100\n",
            "18/18 [==============================] - 8s 425ms/step - loss: 3.1405\n",
            "Epoch 47/100\n",
            "18/18 [==============================] - 7s 387ms/step - loss: 3.1244\n",
            "Epoch 48/100\n",
            "18/18 [==============================] - 8s 464ms/step - loss: 3.1211\n",
            "Epoch 49/100\n",
            "18/18 [==============================] - 7s 371ms/step - loss: 3.0853\n",
            "Epoch 50/100\n",
            "18/18 [==============================] - 8s 455ms/step - loss: 3.0686\n",
            "Epoch 51/100\n",
            "18/18 [==============================] - 8s 473ms/step - loss: 3.0474\n",
            "Epoch 52/100\n",
            "18/18 [==============================] - 8s 432ms/step - loss: 3.0145\n",
            "Epoch 53/100\n",
            "18/18 [==============================] - 8s 433ms/step - loss: 3.0048\n",
            "Epoch 54/100\n",
            "18/18 [==============================] - 7s 378ms/step - loss: 2.9791\n",
            "Epoch 55/100\n",
            "18/18 [==============================] - 8s 457ms/step - loss: 2.9576\n",
            "Epoch 56/100\n",
            "18/18 [==============================] - 7s 372ms/step - loss: 2.9439\n",
            "Epoch 57/100\n",
            "18/18 [==============================] - 8s 464ms/step - loss: 2.9164\n",
            "Epoch 58/100\n",
            "18/18 [==============================] - 7s 368ms/step - loss: 2.8980\n",
            "Epoch 59/100\n",
            "18/18 [==============================] - 8s 458ms/step - loss: 2.8764\n",
            "Epoch 60/100\n",
            "18/18 [==============================] - 7s 372ms/step - loss: 2.8654\n",
            "Epoch 61/100\n",
            "18/18 [==============================] - 8s 459ms/step - loss: 2.8437\n",
            "Epoch 62/100\n",
            "18/18 [==============================] - 8s 450ms/step - loss: 2.8165\n",
            "Epoch 63/100\n",
            "18/18 [==============================] - 7s 394ms/step - loss: 2.7966\n",
            "Epoch 64/100\n",
            "18/18 [==============================] - 8s 458ms/step - loss: 2.7832\n",
            "Epoch 65/100\n",
            "18/18 [==============================] - 7s 378ms/step - loss: 2.7575\n",
            "Epoch 66/100\n",
            "18/18 [==============================] - 8s 460ms/step - loss: 2.7308\n",
            "Epoch 67/100\n",
            "18/18 [==============================] - 7s 367ms/step - loss: 2.7300\n",
            "Epoch 68/100\n",
            "18/18 [==============================] - 8s 464ms/step - loss: 2.6849\n",
            "Epoch 69/100\n",
            "18/18 [==============================] - 7s 383ms/step - loss: 2.6866\n",
            "Epoch 70/100\n",
            "18/18 [==============================] - 8s 448ms/step - loss: 2.6519\n",
            "Epoch 71/100\n",
            "18/18 [==============================] - 8s 423ms/step - loss: 2.6356\n",
            "Epoch 72/100\n",
            "18/18 [==============================] - 7s 389ms/step - loss: 2.6241\n",
            "Epoch 73/100\n",
            "18/18 [==============================] - 8s 468ms/step - loss: 2.6002\n",
            "Epoch 74/100\n",
            "18/18 [==============================] - 7s 374ms/step - loss: 2.5892\n",
            "Epoch 75/100\n",
            "18/18 [==============================] - 8s 452ms/step - loss: 2.5710\n",
            "Epoch 76/100\n",
            "18/18 [==============================] - 7s 364ms/step - loss: 2.5428\n",
            "Epoch 77/100\n",
            "18/18 [==============================] - 8s 460ms/step - loss: 2.5282\n",
            "Epoch 78/100\n",
            "18/18 [==============================] - 7s 368ms/step - loss: 2.5105\n",
            "Epoch 79/100\n",
            "18/18 [==============================] - 10s 548ms/step - loss: 2.4940\n",
            "Epoch 80/100\n",
            "18/18 [==============================] - 8s 467ms/step - loss: 2.4763\n",
            "Epoch 81/100\n",
            "18/18 [==============================] - 7s 375ms/step - loss: 2.4445\n",
            "Epoch 82/100\n",
            "18/18 [==============================] - 8s 464ms/step - loss: 2.4257\n",
            "Epoch 83/100\n",
            "18/18 [==============================] - 7s 373ms/step - loss: 2.4150\n",
            "Epoch 84/100\n",
            "18/18 [==============================] - 8s 460ms/step - loss: 2.3984\n",
            "Epoch 85/100\n",
            "18/18 [==============================] - 7s 379ms/step - loss: 2.3784\n",
            "Epoch 86/100\n",
            "18/18 [==============================] - 8s 446ms/step - loss: 2.3629\n",
            "Epoch 87/100\n",
            "18/18 [==============================] - 8s 440ms/step - loss: 2.3421\n",
            "Epoch 88/100\n",
            "18/18 [==============================] - 7s 390ms/step - loss: 2.3259\n",
            "Epoch 89/100\n",
            "18/18 [==============================] - 8s 461ms/step - loss: 2.3085\n",
            "Epoch 90/100\n",
            "18/18 [==============================] - 7s 374ms/step - loss: 2.2887\n",
            "Epoch 91/100\n",
            "18/18 [==============================] - 8s 460ms/step - loss: 2.2618\n",
            "Epoch 92/100\n",
            "18/18 [==============================] - 7s 374ms/step - loss: 2.2535\n",
            "Epoch 93/100\n",
            "18/18 [==============================] - 8s 459ms/step - loss: 2.2309\n",
            "Epoch 94/100\n",
            "18/18 [==============================] - 7s 377ms/step - loss: 2.2151\n",
            "Epoch 95/100\n",
            "18/18 [==============================] - 8s 435ms/step - loss: 2.1986\n",
            "Epoch 96/100\n",
            "18/18 [==============================] - 8s 420ms/step - loss: 2.1881\n",
            "Epoch 97/100\n",
            "18/18 [==============================] - 7s 389ms/step - loss: 2.1661\n",
            "Epoch 98/100\n",
            "18/18 [==============================] - 8s 462ms/step - loss: 2.1463\n",
            "Epoch 99/100\n",
            "18/18 [==============================] - 7s 371ms/step - loss: 2.1268\n",
            "Epoch 100/100\n",
            "18/18 [==============================] - 8s 473ms/step - loss: 2.1250\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7e0ac96386a0>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_model = tf.keras.models.Model(encoder_inputs, encoder_states)"
      ],
      "metadata": {
        "id": "gzCobspRODZC"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_state_input_h = tf.keras.layers.Input(shape=(200 ,))\n",
        "decoder_state_input_c = tf.keras.layers.Input(shape=(200 ,))\n",
        "\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "decoder_outputs, state_h, state_c = decoder_lstm(\n",
        "    decoder_embedding , initial_state=decoder_states_inputs)\n",
        "decoder_states = [state_h, state_c]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "decoder_model = tf.keras.models.Model(\n",
        "    [decoder_inputs] + decoder_states_inputs,\n",
        "    [decoder_outputs] + decoder_states)"
      ],
      "metadata": {
        "id": "rk0BkC00QCR2"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_input(input_sentence):\n",
        "    tokens = input_sentence.lower().split()\n",
        "    tokens_list = []\n",
        "    for word in tokens:\n",
        "        tokens_list.append(tokenizer.word_index[word])\n",
        "    return preprocessing.sequence.pad_sequences([tokens_list] , maxlen=maxlen_questions , padding='post')"
      ],
      "metadata": {
        "id": "-rbuYuKtPpy5"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tests = ['Hello', 'Are you a bot', 'What is your name', 'That is a very long name', 'see you later']\n",
        "\n",
        "for i in range(5):\n",
        "    states_values = encoder_model.predict(preprocess_input(tests[i]))\n",
        "    empty_target_seq = np.zeros((1 , 1))\n",
        "    empty_target_seq[0, 0] = tokenizer.word_index['start']\n",
        "    stop_condition = False\n",
        "    decoded_translation = ''\n",
        "\n",
        "    while not stop_condition :\n",
        "        dec_outputs , h , c = decoder_model.predict([empty_target_seq] + states_values)\n",
        "        sampled_word_index = np.argmax(dec_outputs[0, -1, :])\n",
        "        sampled_word = None\n",
        "\n",
        "        for word , index in tokenizer.word_index.items() :\n",
        "            if sampled_word_index == index :\n",
        "                decoded_translation += f' {word}'\n",
        "                sampled_word = word\n",
        "\n",
        "        if sampled_word == 'end' or len(decoded_translation.split()) > maxlen_answers:\n",
        "            stop_condition = True\n",
        "\n",
        "        empty_target_seq = np.zeros((1 , 1))\n",
        "        empty_target_seq[0 , 0] = sampled_word_index\n",
        "        states_values = [h , c]\n",
        "    print(f'Human: {tests[i]}')\n",
        "    print()\n",
        "    decoded_translation = decoded_translation.split(' end')[0]\n",
        "    print(f'Bot: {decoded_translation}')\n",
        "    print('-'*25)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AVJ7BH1gP62Q",
        "outputId": "e3f61be8-237b-41cc-91c4-6609fc6a88b4"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Human: Hello\n",
            "\n",
            "Bot:  i am not really really stock\n",
            "-------------------------\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Human: Are you a bot\n",
            "\n",
            "Bot:  i am not just as myself as i should\n",
            "-------------------------\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "Human: What is your name\n",
            "\n",
            "Bot:  i am a lot of robots\n",
            "-------------------------\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Human: That is a very long name\n",
            "\n",
            "Bot:  the moon is a computer about the computer to be\n",
            "-------------------------\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Human: see you later\n",
            "\n",
            "Bot:  i am not really into a human\n",
            "-------------------------\n"
          ]
        }
      ]
    }
  ]
}