{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "7d1d63c7",
      "metadata": {
        "id": "7d1d63c7"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "56e231f4",
      "metadata": {
        "id": "56e231f4"
      },
      "outputs": [],
      "source": [
        "sentences = [\n",
        "    \"I watch anime\",\n",
        "    \"Deep Learning is part of AI\",\n",
        "    \"OnePiece is my favourite Anime\",\n",
        "    \"Generative AI is new trend in the market\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "31300652",
      "metadata": {
        "id": "31300652"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer(num_words=50,oov_token=\"<UNK>\") #nums_of_word will not influence the word index\n",
        "#instead it will effect the text sequence\n",
        "#it will only be trained on num_words-1 words, rest all will be neglected\n",
        "#If you don’t use a token for out of vocabulary(oov_token) words, The word isn’t encoded, and is skipped in the sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "c1a37519",
      "metadata": {
        "id": "c1a37519"
      },
      "outputs": [],
      "source": [
        "tokenizer.fit_on_texts(sentences)  #used to tokenize the list of sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "7cbfd41e",
      "metadata": {
        "id": "7cbfd41e"
      },
      "outputs": [],
      "source": [
        "index = tokenizer.word_index  #returns dictionary of the word index based on the most occured words"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3wvG-ORZQBER",
        "outputId": "831c0932-6b39-4446-c289-15e9eb758f5e"
      },
      "id": "3wvG-ORZQBER",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'<UNK>': 1,\n",
              " 'is': 2,\n",
              " 'anime': 3,\n",
              " 'ai': 4,\n",
              " 'i': 5,\n",
              " 'watch': 6,\n",
              " 'deep': 7,\n",
              " 'learning': 8,\n",
              " 'part': 9,\n",
              " 'of': 10,\n",
              " 'onepiece': 11,\n",
              " 'my': 12,\n",
              " 'favourite': 13,\n",
              " 'generative': 14,\n",
              " 'new': 15,\n",
              " 'trend': 16,\n",
              " 'in': 17,\n",
              " 'the': 18,\n",
              " 'market': 19}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "2a8c0d64",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2a8c0d64",
        "outputId": "bceb8b6a-83be-4e8d-9a8b-34d179b986d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[5, 6, 3], [7, 8, 2, 9, 10, 4], [11, 2, 12, 13, 3], [14, 4, 2, 15, 16, 17, 18, 19]]\n"
          ]
        }
      ],
      "source": [
        "seq = tokenizer.texts_to_sequences(sentences) #used to encode a list of sentences\n",
        "print(seq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "132e7663",
      "metadata": {
        "id": "132e7663"
      },
      "outputs": [],
      "source": [
        "new = [\"Generative AI and Deep Learning is way to upskill in AI\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "a50c5d04",
      "metadata": {
        "id": "a50c5d04"
      },
      "outputs": [],
      "source": [
        "find_new_seq = tokenizer.texts_to_sequences(new)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "ddf0c409",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddf0c409",
        "outputId": "e45bf8ec-ba7a-4168-c5a9-e99feb4640a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[14, 4, 1, 7, 8, 2, 1, 1, 1, 17, 4]]\n"
          ]
        }
      ],
      "source": [
        "print(find_new_seq)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "padded = pad_sequences(find_new_seq,truncating=\"post\",padding=\"post\", maxlen=30)"
      ],
      "metadata": {
        "id": "v8dimbw-Qfi6"
      },
      "id": "v8dimbw-Qfi6",
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "padded"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JXTjLIKqQljG",
        "outputId": "a59216d4-2dea-43ea-8261-99943789121e"
      },
      "id": "JXTjLIKqQljG",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[14,  4,  1,  7,  8,  2,  1,  1,  1, 17,  4,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]],\n",
              "      dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "8f239dd1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8f239dd1",
        "outputId": "0772f47a-0d51-487a-f104-012a27e8c5a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Word Index =  {'<UNK>': 1, 'is': 2, 'anime': 3, 'ai': 4, 'i': 5, 'watch': 6, 'deep': 7, 'learning': 8, 'part': 9, 'of': 10, 'onepiece': 11, 'my': 12, 'favourite': 13, 'generative': 14, 'new': 15, 'trend': 16, 'in': 17, 'the': 18, 'market': 19}\n",
            "\n",
            "Test Sequence =  [[5, 1, 1, 12, 1], [12, 1, 1, 12, 1]]\n",
            "\n",
            "Padded Test Sequence: \n",
            "[[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  5  1  1 12  1]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 12  1  1 12  1]]\n"
          ]
        }
      ],
      "source": [
        "test_data = [\n",
        "    'i really love my dog',\n",
        "    'my dog loves my manatee'\n",
        "]\n",
        "\n",
        "# Generate the sequences\n",
        "test_seq = tokenizer.texts_to_sequences(test_data)\n",
        "\n",
        "# Print the word index dictionary\n",
        "print(\"\\nWord Index = \" , index)\n",
        "\n",
        "# Print the sequences with OOV\n",
        "print(\"\\nTest Sequence = \", test_seq)\n",
        "\n",
        "# Print the padded result\n",
        "padded = pad_sequences(test_seq,truncating=\"post\", maxlen=20)\n",
        "print(\"\\nPadded Test Sequence: \")\n",
        "print(padded)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B7V4b2LxQ6yw"
      },
      "id": "B7V4b2LxQ6yw",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}